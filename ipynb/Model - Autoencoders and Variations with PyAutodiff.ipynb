{
 "metadata": {
  "name": "Model - Autoencoders and Variations with PyAutodiff"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Auto-Encoders and Denoising Auto-Encoders\n",
      "=========================================\n",
      "\n",
      "The auto-encoder (AE) is a classic unsupervised algorithm for non-linear dimensionality reduction.\n",
      "The de-noising auto-encoder (DAE) is an extension of the auto-encoder introduced as a building block for deep networks in [Vincent08].\n",
      "This tutorial introduces the autoencoder as an unsupervised feature learning algorithm for the MNIST data set, and then develops the denoising autoencoder.\n",
      "See section 4.6 of [Bengio09]_ for an overview of auto-encoders.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Before we get started with the rest of the tutorial, \n",
      "# run this once (and again after every kernel restart)\n",
      "# to bring in all the external symbols we'll need.\n",
      "\n",
      "from functools import partial\n",
      "import logging\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import numpy as np\n",
      "from numpy import dot, exp, log, newaxis, sqrt, tanh \n",
      "rng = np.random.RandomState(123)\n",
      "\n",
      "from skdata import mnist, cifar10, svhn\n",
      "import autodiff\n",
      "\n",
      "from util import show_filters\n",
      "print '-> Imports Complete'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's also import the MNIST data set here, so that the examples in the tutorial have some data to work with. (HINT: Some of the exercises will involve using different data sets -- you can use different data sets with the tutorial code by writing a code block like this one that redefining `x` and `x_img_res`.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dtype = 'float32'   # -- this sets the working precision for data and model\n",
      "n_examples = 10000  # -- use up to 50000 examples\n",
      "\n",
      "data_view = mnist.views.OfficialImageClassification(x_dtype=dtype)\n",
      "x_as_images = data_view.train.x[:n_examples]\n",
      "x_img_res = x_as_images.shape[1:3]         # -- handy for showing filters\n",
      "x = x_as_images.reshape((n_examples, -1))  # -- rasterize images to vectors\n",
      "n_visible = x.shape[1]\n",
      "\n",
      "# -- uncomment this line to see the initial weight values\n",
      "show_filters(x[:100], x_img_res, (10, 10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Autoencoders\n",
      "\n",
      "Generally speaking, an autoencoder maps vector input $\\mathbf{x}$ to some intermediate representation $\\mathbf{y}$ and then back into the original space, in such a way that the end-point is close to $\\mathbf{x}$. While the goal thus defined is simply to learn an identity function,\n",
      "things get interesting when the mappings are parameterized or constrained in such a way that a general identity function is either impossible to represent or at least difficult to learn from data. When this is the case, the goal of learning is to learn a _special-purpose_ identity function that _typically_ works for vectors $\\mathbf{x}$ that we care about, which come from some empirically interesting distribution.  The $\\mathbf{y}$ vector that comes out of this process contains all the _important_ information about $x$ in a new and potentially useful way.\n",
      "\n",
      "In our tutorial here we will deal with vectors $\\mathbf{x}$ that come from the MNIST data set of hand-written digits.\n",
      "Examples from MNIST are vectors $\\mathbf{x} \\in [0,1]^N$,\n",
      "and we will look at the classic one-layer sigmoidal autoencoder parameterized by:\n",
      "\n",
      "* matrix $W \\in \\mathbb{R}^{N \\times M}$ - the _weights_\n",
      "* vector $\\mathbf{b} \\in \\mathbb{R}^M$ - the _bias_\n",
      "* matrix $V \\in \\mathbb{R}^{M \\times N}$ - the _reconstruction weights_\n",
      "* vector $\\mathbf{c} \\in \\mathbb{R}^N$ - the _recontruction bias_\n",
      "\n",
      "which encodes vectors $\\mathbf{x}$ into $\\mathbf{y} \\in [0,1]^M$ by the deterministic mapping\n",
      "\n",
      "$$ \\mathbf{h} = s(\\mathbf{x}\\mathbf{W} + \\mathbf{b}) $$\n",
      "\n",
      "Where $s$ is a poinwise sigmoidal function $s(u) = 1 / (1 + e^{-u})$.\n",
      "The latent representation $\\mathbf{h}$,\n",
      "or _code_ is then mapped back (_decoded_) into\n",
      "_reconstruction_ $\\mathbf{z}$ through the similar transformation\n",
      "\n",
      "$$\\mathbf{z} = s(\\mathbf{h}\\mathbf{V} + \\mathbf{c}) $$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run this cell to define the symbols\n",
      "def logistic(u):\n",
      "    \"\"\"Return logistic sigmoid of float or ndarray `u`\"\"\"\n",
      "    return 1.0 / (1.0 + exp(-u))\n",
      "\n",
      "def autoencoder(W, b, V, c, x):\n",
      "    h = logistic(dot(x, W) + b)\n",
      "    # -- using w.T here is called using \"tied weights\"\n",
      "    # -- using a second weight matrix here is called \"untied weights\"\n",
      "    z = logistic(dot(h, V) + c)\n",
      "    return z, h\n",
      "\n",
      "print '-> defined model family'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Training an Autoencoder\n",
      "\n",
      "Training an autoencoder consists of minimizing a reconstruction cost,\n",
      "taking $\\mathbf{z}$ as the reconstruction of $\\mathbf{x}$.\n",
      "Intuitively, we want a model that reconstructs the important aspects of $\\mathbf{x}$\n",
      "so that we guarantee that these aspects are preserved by $\\mathbf{y}$.\n",
      "Typically, mathematical formalizations of this intuition are not particularly\n",
      "exotic, but they should, at the very least, respect the domain and range of $\\mathbf{x}$\n",
      "and $\\mathbf{z}$ respectively.\n",
      "So for example, if $\\mathbf{x}$ were a real-valued element of $\\mathbb{R}^N$ then\n",
      "we might formalize the reconstruction cost as \n",
      "$|| \\mathbf{x} - \\mathbf{z} ||^2$, but there is no mathematical requirement\n",
      "to use the $\\ell_2$ norm or indeed to use a valid norm at all.\n",
      "\n",
      "For our MNIST data, we will suppose that $\\mathbf{x}$ is a vector Bernoulli probabilities,\n",
      "and define the reconstruction cost to be the *cross-entropy* betwen $\\mathbf{z}$ and $\\mathbf{x}$:\n",
      "\n",
      "$$\n",
      "L(\\mathbf{x}, \\mathbf{z}) = - \\sum^d_{k=1}[\\mathbf{x}_k \\log\n",
      "          \\mathbf{z}_k + (1 - \\mathbf{x}_k)\\log(1 - \\mathbf{z}_k)]\n",
      "$$\n",
      "\n",
      "We write this in Python as:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cross_entropy(x, z):\n",
      "    # -- N.B. this is numerically bad, we're counting on Theano to fix up\n",
      "    return -(x * log(z) + (1 - x) * log(1 - z)).sum(axis=1)\n",
      "\n",
      "\n",
      "def training_criterion(W, b, V, c, x):\n",
      "    z, h = autoencoder(W, b, V, c, x)\n",
      "    L = cross_entropy(x, z).mean()\n",
      "    return L\n",
      "\n",
      "print '-> defined training_criterion'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Training an autoencoder is an optimization problem that is\n",
      "typically treated in two steps:\n",
      "\n",
      "1. **Initialization** - choosing a starting point for gradient-based search,\n",
      "2. **Local Search** - running a gradient-based optimization algorithm\n",
      "\n",
      "**Initialization**\n",
      "The parameters of this particular model\n",
      "($W, \\mathbf{b}, V, \\mathbf{c}$) must be initialized\n",
      "so that symmetry is broken between the columns of $W$ (also the rows of $V$),\n",
      "otherwise the gradient on each column will be identical and local search will\n",
      "be completely ineffective.\n",
      "\n",
      "It is customary to initialize $W$ to small random values to break symmetry,\n",
      "and to initialize the rest of the parameters to 0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- allocate and initialize a new model  (w, visbias, hidbias)\n",
      "\n",
      "# -- tip: choose the number of hidden units as a pair, so that show_filters works\n",
      "n_hidden2 = (10, 10)\n",
      "n_hidden = np.prod(n_hidden2)\n",
      "W = rng.uniform(\n",
      "        low=-4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
      "        high=4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
      "        size=(n_visible, n_hidden)).astype(dtype)\n",
      "V = np.zeros_like(W.T)\n",
      "b = np.zeros(n_hidden).astype(dtype)\n",
      "c = np.zeros(n_visible).astype(dtype)\n",
      "\n",
      "# -- uncomment this line to see the initial weight values\n",
      "show_filters(W.T, x_img_res, n_hidden2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Local Search**\n",
      "\n",
      "The training criterion $L$ defined above is a deterministic and differentiable function of parameters ($W, b, V, c$) so any multi-dimensional minimization algorithm can do the job.\n",
      "The speed and accuracy of a the method generally depends on the size nature of the data set, the parameters and parameterization of the model, and of course the sophistication of the minimization algorithm.\n",
      "\n",
      "A classic, and still widely useful, algorithm is stochastic gradient descent.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t0 = time.time()\n",
      "online_batch_size = 1 # -- =1 for \"pure online\", >1 for \"minibatch\"\n",
      "streams={'x': x.reshape((\n",
      "                         n_examples / online_batch_size,\n",
      "                         online_batch_size,\n",
      "                         n_visible))}\n",
      "W, b, V, c = autodiff.fmin_sgd(training_criterion,\n",
      "        args=(W, b, V, c),\n",
      "        streams=streams,\n",
      "        stepsize=0.001,\n",
      "        loops=1,  # -- fmin_sgd can iterate over the streams repeatedly\n",
      "        print_interval=1000,\n",
      "        )\n",
      "\n",
      "show_filters(W.T, x_img_res, n_hidden2)\n",
      "print 'Online training took %f seconds' % (time.time() - t0)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another powerful minimization method is the well-known (L-BFGS) algorithm. When used as a training algorithm, it is called a _batch_ algorithm because it does not deal well with stochastic functions, and therefore requires that we evaluate the total loss over all training examples on each cost function evaluation.\n",
      "\n",
      "L-BFGS can be run right away from the random initialization,\n",
      "but often what works better is to do a few loops of `fmin_sgd` to move the initial random parameters to a promising area of the search space, and then to refine that solution with L-BFGS."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- BATCH TRAINING\n",
      "def batch_criterion(W, b, V, c):\n",
      "    return training_criterion(W, b, V, c, x)\n",
      "\n",
      "W, b, V, c = autodiff.fmin_l_bfgs_b(batch_criterion,\n",
      "        args=(W, b, V, c),\n",
      "        maxfun=20,    # -- how many function calls to allow?\n",
      "        iprint=1,     # -- 1 for verbose, 0 for normal, -1 for quiet\n",
      "        m=20)         # -- how well to approximate the Hessian\n",
      "\n",
      "show_filters(W.T, x_img_res, n_hidden2) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's it, we've trained an auto-encoder.\n",
      "\n",
      "### Autoencoder Summary\n",
      "\n",
      "An autoencoder is a function that maps a vector to approximately itself, by passing it through intermediate values in such a way that the pure identify function is difficult or impossible.\n",
      "An autoencoder is a non-stochastic pre-cursur to several more modern probabilistic models such as De-Noising AutoEncoders, Sparse Coding, and Restricted Boltzmann Machines. They are a flexible class of feature extraction algorithms that can be quick to train by either stochastic gradient descent, or more sophisticated minimization algorithms such as L-BFGS.\n",
      "\n",
      "Spend some time running through the exercises below to get a better feel for how autoencoders work and what they can do.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Linear AutoEncoder\n",
      "\n",
      "If there is one linear hidden layer (the code) and\n",
      "the mean squared error criterion is used to train the network, then the $k$\n",
      "hidden units learn to project the input in the span of the first $k$\n",
      "principal components of the data. If the hidden\n",
      "layer is non-linear, the auto-encoder behaves differently from PCA,\n",
      "with the ability to capture multi-modal aspects of the input\n",
      "distribution. The departure from PCA becomes even more important when\n",
      "we consider *stacking multiple encoders* (and their corresponding decoders)\n",
      "when building a deep auto-encoder [Hinton06]_.\n",
      "\n",
      "Re-inialize the weights, and try SGD on the following linear auto-encoder model.\n",
      "\n",
      "HINT: if you get NaN as the Value during SGD, try lowering the step size to 0, and then slowly raising it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def squared_error(x, x_rec):\n",
      "    \"\"\"Return the squared error of approximating `x` with `x_rec`\"\"\"\n",
      "    return ((x - x_rec) ** 2).sum(axis=1)\n",
      "\n",
      "def linear_autoencoder_criterion(W, c, x):\n",
      "    hid = dot(x - c, W)\n",
      "    x_rec = dot(hid, W.T)\n",
      "    cost = squared_error(x - c, x_rec)\n",
      "    return cost.mean()\n",
      "\n",
      "print linear_autoencoder_criterion(W, c, x[:3])\n",
      "\n",
      "# Modify the SGD training code (or paste it here) to train just W and c\n",
      "# using the linear_autoencoder_criterion\n",
      "# ..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Regularization via Tied Weights\n",
      "\n",
      "The weight matrix $V$ of the reverse mapping may be\n",
      "optionally constrained by $V = W^{T}$, which is\n",
      "called an autoencoder with *tied weights*.\n",
      "This is another way to regularize the autoencoder model.\n",
      "The PCA interpretation of the autoencoder (linear autoencoder), the RBM model (algorithmically similar to the autoencoder), the K-Means, and sparse coding interpretations of the auto-encoder all demand tied weights. How do the filters come out if you use the `training_criterion_tied`?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def training_criterion_tied(W, b, c, x):\n",
      "    return training_criterion(W, b, W.T, c, x)\n",
      "\n",
      "# Now modify (or paste here) an optimization fragment that optimizes only (W, b, c)\n",
      "# ...\n",
      "# How is W changed by being tied to V ?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Capacity Regularization via L2 Regularization\n",
      "\n",
      "One serious potential issue with auto-encoders is that if there is no other\n",
      "constraint besides minimizing the reconstruction error,\n",
      "then an auto-encoder with $n$ inputs and an\n",
      "encoding of dimension at least $n$ could potentially just learn\n",
      "the identity function, and fail to differentiate\n",
      "test examples (from the training distribution) from other input configurations.\n",
      "Surprisingly, experiments reported in [Bengio07]_ nonetheless\n",
      "suggest that in practice, when trained with\n",
      "stochastic gradient descent, non-linear auto-encoders with more hidden units\n",
      "than inputs (called overcomplete) yield useful representations\n",
      "(in the sense of classification error measured on a network taking this\n",
      "representation in input). A simple explanation is based on the\n",
      "observation that stochastic gradient\n",
      "descent with early stopping is similar to an L2 regularization of the\n",
      "parameters. To achieve perfect reconstruction of continuous\n",
      "inputs, a one-hidden layer auto-encoder with non-linear hidden units\n",
      "(exactly like in the above code)\n",
      "needs very small weights in the first (encoding) layer (to bring the non-linearity of\n",
      "the hidden units in their linear regime) and very large weights in the\n",
      "second (decoding) layer.\n",
      "With binary inputs, very large weights are\n",
      "also needed to completely minimize the reconstruction error. Since the\n",
      "implicit or explicit regularization makes it difficult to reach\n",
      "large-weight solutions, the optimization algorithm finds encodings which\n",
      "only work well for examples similar to those in the training set, which is\n",
      "what we want. It means that the representation is exploiting statistical\n",
      "regularities present in the training set, rather than learning to\n",
      "replicate the identity function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The claim above is that SGD already implements an L2 regularization implicitly,\n",
      "# whereas L-BFGS less so or not at all.\n",
      "# Can you detect a difference? (I'm not sure if you can or not)\n",
      "# \n",
      "# One way to explore the question is to let SGD and L-BFGS both run until the reconstruction\n",
      "# error is nearly zero. Can both algorithms get that far? Which one finds larger parameters?\n",
      "#\n",
      "# Now define a new training_criterion function that includes L2 penalty\n",
      "# (W ** 2).sum()\n",
      "#\n",
      "# How does an explicit L2 penalty affect SGD? What about L-BFGS?\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Sparsity, K-means\n",
      "\n",
      "There are different ways that an auto-encoder with more hidden units\n",
      "than inputs could be prevented from learning the identity, and still\n",
      "capture something useful about the input in its hidden representation.\n",
      "One is the addition of sparsity (forcing many of the hidden units to\n",
      "be zero or near-zero), and it has been exploited very successfully\n",
      "by many [Ranzato07]_ [Lee08]_.\n",
      "\n",
      "One of the most extreme forms of sparsity in autoencoders is known as the K-means algorithm.\n",
      "In the K-means model, the latent code `hid` is a vector with one `1` and the rest all `0`.\n",
      "K-means is typically solved using a very efficient batch EM algorithm, but we can also\n",
      "tackle it with the same tools we've been using thus far.\n",
      "\n",
      "How does this compare with [sklearn's K-means solver](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def softmax(x):\n",
      "    \"\"\"Return the softmax of each row in x\"\"\"\n",
      "    x2 = x - x.max(axis=1)[:, newaxis]\n",
      "    ex = exp(x2)\n",
      "    return ex / ex.sum(axis=1)[:, newaxis]\n",
      "\n",
      "# helper functions -- run this once after a kernel restart\n",
      "# Re-run it after any change you make to these routines.\n",
      "\n",
      "def euclidean_distances2(X, Y):\n",
      "    \"\"\"Return all-pairs squared distances between rows of X and Y\n",
      "    \"\"\"\n",
      "    # N.B. sklearn.metrics.pairwise.euclidean_distances\n",
      "    # offers a more robust version of this routine,\n",
      "    # but which does things that autodiff currently does not support.\n",
      "    XX = np.sum(X * X, axis=1)[:, newaxis]\n",
      "    YY = np.sum(Y * Y, axis=1)[newaxis, :]\n",
      "    distances = np.maximum(XX - 2 * dot(X, Y.T) + YY, 0)\n",
      "    return distances\n",
      "    \n",
      "def k_means_real_x(w, x):\n",
      "    xw = euclidean_distances2(x, w.T)\n",
      "    # -- This calculates a hard winner\n",
      "    hid = (xw == xw.min(axis=1)[:, newaxis]).astype(dtype)\n",
      "    # -- This calculates a soft winner\n",
      "    hid = hid + softmax(xw)\n",
      "    x_rec = dot(hid/2, w.T)\n",
      "    cost = ((x - x_rec) ** 2).sum(axis=1)\n",
      "    rval = cost.mean()\n",
      "    return rval\n",
      "\n",
      "if 1:\n",
      "    W, = autodiff.fmin_sgd(k_means_real_x,\n",
      "        args=(W,),\n",
      "        streams=streams,\n",
      "        stepsize=0.001,\n",
      "        loops=2,  # -- fmin_sgd can iterate over the streams repeatedly\n",
      "        print_interval=1000,\n",
      "        )\n",
      "\n",
      "if 0:\n",
      "    W, = autodiff.fmin_l_bfgs_b(lambda W: k_means_real_x(W, x),\n",
      "        args=(W,),\n",
      "        maxfun=60,    # -- how many function calls to allow?\n",
      "        iprint=1,     # -- 1 for verbose, 0 for normal, -1 for quiet\n",
      "        m=20)         # -- how well to approximate the Hessian\n",
      "\n",
      "\n",
      "show_filters(W.T, x_img_res, n_hidden2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Sparsity via sparse coding\n",
      "\n",
      "Sorry guys, this isn't implemented yet.\n",
      "So the exercise is: implement FISTA, or the (Olshausen and Field, 1996) version.\n",
      "\n",
      "* HINT: [sklearn's LASSO]() might help with half of the FISTA algorithm\n",
      "* HINT: Also consider using Leif Johnson's [py-lars code](https://github.com/lmjohns3/py-lars)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FISTA = NotImplementedError\n",
      "# real-real Sparse Coding\n",
      "def sparse_coding_real_x(x, w, hidbias, visbias, sparse_coding_algo=FISTA):\n",
      "    # -- several sparse coding algorithms have been proposed, but they all\n",
      "    # give rise to a feature learning algorithm that looks like this:\n",
      "    hid = sparse_coding_algo(x, w)\n",
      "    x_rec = dot(hid, w.T) + visbias\n",
      "    cost = ((x - x_rec) ** 2).mean(axis=1)\n",
      "    # -- the gradient on this cost wrt `w` through the sparse_coding_algo is\n",
      "    # often ignored. At least one notable exception is the work of Karol\n",
      "    # Greggor.  I feel like the Implicit Differentiation work of Drew Bagnell\n",
      "    # is another, but I'm not sure.\n",
      "    return cost, hid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Denoising AutoEncoder\n",
      "\n",
      "Another variation on the autoencoder theme is to add noise to the input by zeroing out some of the elements randomly. This is known as the denoising autoencoder (Vincent et al., XXX)\n",
      "and it has been shown to be a form of score-matching (probabilistic model interpretation)\n",
      "and a good feature extraction algorithm."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def denoising_autoencoder_binary_x(W, b, c, x):\n",
      "    # -- corrupt the input by zero-ing out some values randomly\n",
      "    noisy_x = x * (rand(*x.shape) > .3)\n",
      "    hid = logistic(dot(noisy_x, W) + b)\n",
      "    x_rec = logistic(dot(hid, W.T) + c)\n",
      "    cost = cross_entropy(x, x_rec)\n",
      "    return cost.mean()\n",
      "\n",
      "W, b, c = autodiff.fmin_sgd(denoising_autoencoder_binary_x,\n",
      "        args=(W, b, c),\n",
      "        streams=streams,\n",
      "        stepsize=0.01,\n",
      "        loops=1,  # -- fmin_sgd can iterate over the streams repeatedly\n",
      "        print_interval=1000,\n",
      "        )\n",
      "\n",
      "show_filters(W.T, x_img_res, n_hidden2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Not really an autoencoder -- the Restricted Boltzmann Machine\n",
      "\n",
      "The Contrastive Divergence algorithm for training Restricted Boltzmann Machines (RBMs) looks a lot like an auto-encoder from an algorithmic perspective, especially the denoising autoencoder. Tempting though it is to use the autoencoder as a unifying framework, it is a stretch to call an RBM an auto-encoder. Mathematically, it is better to train an RBM by Persistent CD (aka Stochastic Maximum Likelihood) and these methods actually work on the basis of ensuring that the RBM does *not* perfectly reconstruct any training instances. Nevertheless,\n",
      "the following code implements the CD algorithm in a way that makes a clear connection to the other autoencoder-style models above.\n",
      "\n",
      "One important difference with the cases above is that\n",
      "_CD is not in fact gradient descent on a cost function_ and although this code makes use of a difference in free energies as a cost function, it is just a trick.\n",
      "Consequently, you'll see that the so-called \"cost\" values regularly jump both up and down even when the algorithm is working perfectly well. Computation of the likelihood of an RBM involves an infamously intractable partition function -- there are techniques for estimating it (see Ruslan Salakhutdinov's PhD thesis), but no tutorial here yet."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rbm_binary_x(w, hidbias, visbias, x):\n",
      "    hid = logistic(dot(x, w) + hidbias)\n",
      "    hid_sample = (hid > rand(*hid.shape)).astype(x.dtype)\n",
      "\n",
      "    # -- N.B. model is not actually trained to reconstruct x\n",
      "    x_rec = logistic(dot(hid_sample, w.T) + visbias)\n",
      "    x_rec_sample = (x_rec > rand(*x_rec.shape)).astype(x.dtype)\n",
      "\n",
      "    # \"negative phase\" hidden unit expectation\n",
      "    hid_rec = logistic(dot(x_rec_sample, w) + hidbias)\n",
      "\n",
      "    def free_energy(xx):\n",
      "        xw_b = dot(xx, w) + hidbias\n",
      "        return -log(1 + exp(xw_b)).sum(axis=1) - dot(xx, visbias)\n",
      "\n",
      "    cost = free_energy(x) - free_energy(x_rec_sample)\n",
      "    return cost.mean()\n",
      "\n",
      "W, b, c = autodiff.fmin_sgd(rbm_binary_x,\n",
      "        args=(W, b, c),\n",
      "        streams=streams,\n",
      "        stepsize=0.01,\n",
      "        loops=5,  # -- fmin_sgd can iterate over the streams repeatedly\n",
      "        print_interval=1000,\n",
      "        )\n",
      "\n",
      "show_filters(W.T, x_img_res, n_hidden2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}