{
 "metadata": {
  "name": "Model - Linear SVM with PyAutodiff"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Linear Support Vector Machine\n",
      "=============================\n",
      "\n",
      "The linear support vector machine (SVM) is a linear classifier parametrized by a matrix of weights $W$ and a bias vector $\\mathbf{b}$.  We will develop the multiclass \"One vs. All\" linear support vector machine, which is a concatentation of two-class support vector machines. We will suppose that our label set is the non-negative integers up to some maximum L.\n",
      "\n",
      "There are other ways of setting up a multi-class SVM, such using the Crammer-Singer loss or\n",
      "[LaRank](http://www.machinelearning.org/proceedings/icml2007/papers/381.pdf).\n",
      "Despite theoretical limitations (lack of consistency!?) the approach is widely used.\n",
      "See [John Langford's page on machine learning reductions](http://hunch.net/~jl/projects/reductions/reductions.html)\n",
      "for more perspectives on multi-class SVMs.\n",
      "\n",
      "Deep architectures for classification are typically formed by classifying learned features with a\n",
      "linear classifier such as an SVM or logistic regression model.\n",
      "We will begin our exploration of deep architectures with the shallowest model of all: the linear SVM.\n",
      "\n",
      "The Deep Learning Tutorials provide a good complementary introduction to \n",
      "[logistic regression](http://deeplearning.net/tutorial/logreg.html)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# initialize the workspace by importing several symbols\n",
      "import logging\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import numpy as np\n",
      "from numpy import arange, dot, maximum, ones, tanh, zeros\n",
      "from numpy.random import randn\n",
      "\n",
      "from skdata import mnist\n",
      "import autodiff\n",
      "\n",
      "try:\n",
      "    from util import show_filters\n",
      "except:\n",
      "    print 'WARNING: show_filters will not work'\n",
      "    def show_filters(*a, **kw): pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- load and prepare the data set (even download if necessary)\n",
      "dtype = 'float32'\n",
      "n_examples = 50000\n",
      "n_classes = 10         # -- denoted L in the math expressions\n",
      "img_shape = (28, 28)\n",
      "\n",
      "data_view = mnist.views.OfficialVectorClassification(x_dtype=dtype)\n",
      "x = data_view.all_vectors[data_view.fit_idxs[:n_examples]]\n",
      "y = data_view.all_labels[data_view.fit_idxs[:n_examples]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Supposing that our input is a vector (generally a _feature vector_) $\\mathbf{x}$, the prediction $l^*$ of the OVA-SVM is the argmax of the affine function:\n",
      "\n",
      "$$\n",
      "l^* = \\operatorname*{argmax}_{l=0}^{L} (\\mathbf{x} W_l + \\mathbf{b}_l)\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ova_svm_prediction(W, b, x):\n",
      "    return np.argmax(np.dot(x, W) + b, axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Training an SVM\n",
      "\n",
      "The most standard way of training an SVM is to maximize the _margin_ on training data.\n",
      "The _margin_ is always defined in the two-class case (where $y \\in \\\\{-1, +1\\\\}$) \n",
      "as $y * (xW + b)$.\n",
      "The affine function parametrized by $W$ and $b$ defines a hyper-plane where $\\mathbf{x}W + b = 0$ that the SVM interprets as a decision surface.\n",
      "The _margin_ represents how far away $\\mathbf{x}W+b$ is from being on the wrong side of the decision surface: large positive values mean there is a safe distance (\"margin of error?\"), negative values mean that the decision surface is actually incorrect for the given $\\mathbf{x}$.\n",
      "\n",
      "The most standard sense in which SVMs maximize margin is via the _hinge loss_. The hinge loss of margin value $u$ is defined as\n",
      "\n",
      "$$\n",
      "\\mathrm{hinge}(u) = \\max(0, 1-u)\n",
      "$$\n",
      "\n",
      "By maximizing the average hinge loss of the margin of training examples, we maximize a tight convex upper bound on the mis-classification rate (zero-one loss), and can \n",
      "get a good binary classifier using fast algorithms for convex optimization."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def hinge(u):\n",
      "    return np.maximum(0, 1 - u)\n",
      "\n",
      "ugrid = np.arange(-5, 5, .1)\n",
      "plot(ugrid, hinge(ugrid), label='hinge loss')\n",
      "plot(ugrid, ugrid < 0, label='zero-one loss')\n",
      "legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Multiclass SVM\n",
      "\n",
      "In the multiclass case, it is not clear what the \"correct\" margin definition should be, and several useful ones have been proposed.\n",
      "For the rest of this tutorial we'll develop the \"one vs. all\" multiclass SVM, sometimes called an OVA-SVM.\n",
      "\n",
      "In the OVA-SVM, the training objective is defined by $L$ independent SVMs.\n",
      "We will train an OVA-SVM by converting the integer-valued labels\n",
      "$y$ to $Y \\in \\\\{-1, +1\\\\}^{N \\times L}$ and training $L$ SVMs at once.\n",
      "The $L$ columns of $Y$ represent binary classification tasks.\n",
      "The $L$ columns of $W$ and $L$ elements of $\\mathbf{b}$ store the parameters of the $L$ SVMs.\n",
      "\n",
      "The [unregularized] training objective is:\n",
      "\n",
      "$$\n",
      "\\mathcal{L}(\\mathcal{D}; W, \\mathbf{b}) =\n",
      "\\frac{1}{N}\n",
      "\\sum_{(\\mathbf{x}^{(i)}, Y^{(i)}) \\in \\mathcal{D}}\n",
      "~\n",
      "\\sum_{l=1}^{L}\n",
      "~\n",
      "\\max \\left( 0, 1 - Y^{(i)}_l (\\mathbf{x}^{(i)} W_l + \\mathbf{b}_l) \\right)\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- prepare a \"1-hot\" version of the labels, denoted Y in the math\n",
      "y1 = -1 * ones((len(y), n_classes)).astype(dtype)\n",
      "y1[arange(len(y)), y] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ova_svm_cost(W, b, x, y1):\n",
      "    # -- one vs. all linear SVM loss\n",
      "    margin = y1 * (dot(x, W) + b)\n",
      "    cost = hinge(margin).mean(axis=0).sum()\n",
      "    return cost\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The training itself consists in minimizing the objective $\\mathcal{L}(\\mathcal{D}; W, b)$ with respect to $W$ and $b$.  The criterion is convex, so it doesn't much matter where we start. Zero works well in practice."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# initialize the model\n",
      "W = zeros((x.shape[1], n_classes), dtype=dtype)\n",
      "b = zeros(n_classes, dtype=dtype)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are many specialized SVM solvers available, but they tend to be most helpful for non-linear SVMs.  In the linear case a simple combination of stochastic gradient descent (SGD) and BFGS can be very effective."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- do n_online_loops passes through the data set doing SGD\n",
      "#    This can be faster at the beginning than L-BFGS\n",
      "t0 = time.time()\n",
      "online_batch_size = 1\n",
      "n_online_epochs = 1\n",
      "n_batches = n_examples / online_batch_size\n",
      "W, b = autodiff.fmin_sgd(ova_svm_cost, (W, b),\n",
      "            streams={\n",
      "                'x': x.reshape((n_batches, online_batch_size, x.shape[1])),\n",
      "                'y1': y1.reshape((n_batches, online_batch_size, y1.shape[1]))},\n",
      "            loops=n_online_epochs,\n",
      "            step_size=0.001,\n",
      "            print_interval=1000,\n",
      "            )\n",
      "print 'SGD took %.2f seconds' % (time.time() - t0)\n",
      "show_filters(W.T, img_shape, (2, 5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -- L-BFGS optimization of our SVM cost.\n",
      "\n",
      "def batch_criterion(W, b):\n",
      "    return ova_svm_cost(W, b, x, y1)\n",
      "\n",
      "W, b = autodiff.fmin_l_bfgs_b(batch_criterion, (W, b), maxfun=20, m=20, iprint=1)\n",
      "\n",
      "print 'final_cost', batch_criterion(W, b)\n",
      "# -- N. B. the output from this command comes from Fortran, so iPython does not see it.\n",
      "#    To monitor progress, look at the terminal from which you launched ipython\n",
      "show_filters(W.T, img_shape, (2, 5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Testing the SVM\n",
      "\n",
      "Once the a classifier has been trained, we can test it for generalization accuracy on the test set. We test it by making predictions for the examples in the test set and counting up the number of classification mistakes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_predictions = ova_svm_prediction(W, b, x)\n",
      "train_errors = y != train_predictions\n",
      "print 'Current train set error rate', np.mean(train_errors)\n",
      "\n",
      "test_predictions = ova_svm_prediction(W, b, data_view.all_vectors[data_view.tst_idxs])\n",
      "test_errors = data_view.all_labels[data_view.tst_idxs] != test_predictions\n",
      "print 'Current test set error rate', np.mean(test_errors)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## SVM Summary\n",
      "\n",
      "The linear Support Vector Machine (SVM) is an accurate and quick-to-train linear classifier.\n",
      "We looked a multiclass variant called a \"One vs. All\" SVM,\n",
      "parameterized by a matrix $W$ of weights and a vector $\\mathbf{b}$ of biases.\n",
      "A linear SVM can be trained by gradient descent;\n",
      "whether SGD or L-BFGS works faster depends on the size of the data set and accuracy desired from the minimization process. A few iterations of SGD followed by refinement by L-BFGS is a fairly robust strategy.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: L1 and L2 regularization\n",
      "\n",
      "Typically linear SVMs are L2-regularized. That means that in addition to $\\mathcal{L}$, we minimize the sum of the squared elements of $W$:\n",
      "\n",
      "$$\n",
      "\\mathcal{L}_{\\ell_2}(\\mathcal{D}; W, b) = \\mathcal{L}(\\mathcal{D}; W, b) + \\alpha ||W||_2^2\n",
      "$$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ova_svm_cost_l2(W, b, x):\n",
      "    raise NotImplementedError('implement me')\n",
      "\n",
      "# re-run the SGD and LBFGS training fragments after filling\n",
      "# in this function body to implement an L2-regularized SVM.\n",
      "\n",
      "# In cases where the training data are linearly separable, this can be very important.\n",
      "# How big does \\alpha have to be to make any difference?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Data Z-Normalization\n",
      "\n",
      "Although the optimization of an SVM is a convex problem, the efficiency and stopping criteria of both SGD and L-BFGS are generally improved by centering and normalizing each input feature beforehand. The procedure is simple: subtract off the training set mean of each feature and divide by the standard deviation.\n",
      "\n",
      "How does this affect the behavior of SGD and L-BFGS?\n",
      "\n",
      "How should you deal with features whose standard deviation is very small? What about when it is 0?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Different Data Sets\n",
      "\n",
      "We have been using MNIST so far. How good is a linear classifier on other datasets that are typically used in deep learning?\n",
      "The following two code fragments bring in access to the CIFAR-10 and SVHN data sets.\n",
      "You'll have to access the `shape` attribute of the training images to figure out how many visible units there are, and then re-allocate `W` before repeating the training and testing processes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from skdata import cifar10\n",
      "n_examples = 10000\n",
      "# -- RECOMMENDED: restart the IPython kernel to clear out memory\n",
      "data_view = cifar10.view.OfficialVectorClassification(x_dtype='float32', n_train=n_examples)\n",
      "\n",
      "# Write the rest of the answer here:\n",
      "# - define x and y for training\n",
      "\n",
      "# Repeat the training steps in the code fragments above to train a linear SVM for CIFAR10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from skdata import svhn\n",
      "n_examples = 10000\n",
      "# -- RECOMMENDED: restart the IPython kernel to clear out memory\n",
      "data_view = svhn.view.CroppedDigitsView2(x_dtype='float32', n_train=n_examples)\n",
      "\n",
      "# Write the rest of the answer here:\n",
      "# - define x and y for training\n",
      "\n",
      "# Repeat the training steps in the code fragments above to train a linear SVM for\n",
      "# Street View House Numbers (SVHN)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bonus: Pixel colors in cifar10 and svhn are encoded as RGB triples.  There are many other ways of representing color. Some of these are linearly related to RGB, others non-linearly.\n",
      "The [scikit-image](http://scikits-image.org/) project defines a number of color-conversion routines.\n",
      "Are any of these color representations better than RGB for image classification?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from skimage.color import convert_colorspace\n",
      "\n",
      "# Write your answer here\n",
      "# use convert_colorspace to define new versions of the training and testing images.\n",
      "\n",
      "# Hint - you'll have to un-rasterize the x matrix into a N-dimensional array\n",
      "#        n. examples x n. rows x n. cols x n. channels\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Variations on the Hinge Loss\n",
      "\n",
      "You don't have to use the hinge loss to train a linear classifier. Other popular loss functions include the squared hinge loss, the \"Huberized\" hinge loss, and the exponential function.  These functions are continuously differentiable, which you might expect to help for gradient-based optimization. Does it help? How else are these functions different? In what cases would it matter?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Write your answer here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: Crammer-Singer Loss for Multiclass SVM\n",
      "\n",
      "TODO: VERIFY THIS QUESTION\n",
      "\n",
      "The One vs. All SVM suffers, in theory, from a lack of Bayes consistency.\n",
      "The _Crammer-Singer loss_ defines the multiclass training objective differently, in a way that is Bayes-consistent.\n",
      "\n",
      "To define the Crammer-Singer loss let's define\n",
      "$\\mathbf{h} = \\mathbf{x}W + \\mathbf{b}$,\n",
      "and look at $\\mathbf{m}$ where \n",
      "$\\mathbf{m}_j = 1 - \\mathbf{h}_l - \\mathbf{h}_j $\n",
      "for all elements $j$ that are not the correct label $l$.\n",
      "\n",
      "The Crammer-Singer objective is to minimize the expectation over the dataset of\n",
      "$\\sum_j max(0, \\mathbf{m}_{j})$.\n",
      "\n",
      "Can you code this up as a training criterion and use it?\n",
      "\n",
      "HINT: Start with the case of pure stochastic gradient descent, I think the batch case requires numpy's _advanced indexing_ which not currently supported by `autodiff`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Write your answer here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}